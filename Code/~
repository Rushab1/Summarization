import argparse
import os, re, sys
import numpy as np
import json
import pickle
import random
from scipy.stats import spearmanr, pearsonr
from tqdm import tqdm
import hashlib
from IPython import embed
from copy import deepcopy
import multiprocessing as mp
import argparse

PER_TEST_FILES = 35.0
PER_VAL_FILES = 5.0
DOMAINS = ["Business", "Sports", "Science", "USIntlRelations"]

#Makes a file of all training and test file locations for every year given the news type
#Saved at USE_Data_files/<Year>/train_file_list.txti and USE_Data_files/<year>/test_file_list.txt
def get_file_lists_domain(data_dir, domain):
    file_list = []
    test_file_list = []

    for root, subdir, files in os.walk(os.path.join(data_dir, domain)):
        for filename in files:
            file_list.append(os.path.join(root, filename))

    random.shuffle(file_list)

    num_files = len(file_list)
    num_test_files = int(PER_TEST_FILES/100.0 * num_files)
    num_val_files = int(PER_VAL_FILES/100.0 * num_files)

    test_file_list = file_list[-num_test_files:]
    file_list = file_list[ : (num_files - num_test_files) ]
    num_files = len(file_list)

    val_file_list = file_list[-num_val_files:]
    file_list = file_list[ : (num_files - num_val_files) ]
    num_files = len(file_list)

    print("{}\nTrain files: {}\n val Files: {}\nTest Files: {}\n".format(
                    domain, str(num_files), str(num_val_files), str(num_test_files) ))

    return file_list, val_file_list, test_file_list

def write_file_lists(data_dir, file_list, val_file_list, test_file_list):
    f = open(os.path.join(data_dir, "train_file_list.txt"), "w")
    f.write("\n".join(file_list))
    f.close()

    f = open(os.path.join(data_dir, "test_file_list.txt"), "w")
    f.write("\n".join(test_file_list))
    f.close()

    f = open(os.path.join(data_dir, "val_file_list.txt"), "w")
    f.write("\n".join(val_file_list))
    f.close()

#Some files may occur in more than one domains but we treat them as different files
def get_file_list(data_dir, dataset):
    global DOMAINS

    #Create necessary directories
    if not os.path.exists("../Data/Processed_Data/"):
        os.mkdir("../Data/Processed_Data/")

    if not os.path.exists("../Data/Processed_Data/" + dataset):
        os.mkdir("../Data/Processed_Data/" + dataset)

    all_file_list = []
    all_val_file_list = []
    all_test_file_list = []

    for domain in DOMAINS:
        file_list, val_file_list, test_file_list = get_file_lists_domain(data_dir, domain)

        all_file_list.extend(file_list)
        all_val_file_list.extend(val_file_list)
        all_test_file_list.extend(test_file_list)

        if not os.path.exists("../Data/Processed_Data/" + dataset + "/" + domain):
            os.mkdir("../Data/Processed_Data/" + dataset + "/" + domain)

        write_file_lists("../Data/Processed_Data/" + dataset + "/" + domain + "/",
                            file_list,
                            val_file_list,
                            test_file_list)
    domain = "All"
    if not os.path.exists("../Data/Processed_Data/" + dataset + "/" + domain):
        os.mkdir("../Data/Processed_Data/" + dataset + "/" + domain)

    write_file_lists("../Data/Processed_Data/" + dataset + "/" + domain + "/",
                        all_file_list,
                        all_val_file_list,
                        all_test_file_list)

    print("{}\nTrain files: {}\n val Files: {}\nTest Files: {}\n".format(
        domain, str(len(all_file_list)), str(len(all_val_file_list)), str(len(all_test_file_list)) ))

#parallely reading json files and extracting sentences
def json_sentence_extractor(file_list, JobQueue):
    Sentences = []
    local_sent = []

    for fname in tqdm(file_list):
        f = open(fname)
        try:
            f_pairs = json.load(f)
        except Exception as e:
            print("Cannot load json: " + fname + ": Error: " + str(e))
            continue

        for dct in f_pairs:
            source = dct['source']
            target = dct['target']
            local_sent.append(source)
            local_sent.append(target)

        local_sent = list(set(local_sent))
        Sentences.extend(local_sent)
        local_sent = []
    JobQueue.put(Sentences)

def json_sentence_writer(savefile, JobQueue):
    f = open(savefile, "w")
    while(True):
        res = JobQueue.get()
        if res == "kill":
            break

        for i in range(0, len(res)):
            f.write(res[i] + "\n")
    f.close()

#Extracts all sentences from the dataset and dumps into a text file, this file is typically large ~1GB
def get_sentences(dataset, parallelism = 10):
    print("Extracting sentences")

    data_dir = os.path.join("../Data/Processed_Data/", dataset)
    save_dir = os.path.join(data_dir , "pkl_files")

    if not os.path.exists(save_dir):
        os.mkdir(save_dir)

    #get list of all dataset files
    file_list = open(os.path.join(data_dir, "All/train_file_list.txt")).read().strip().split("\n")
    file_list.extend(open(os.path.join(data_dir, "All/val_file_list.txt")).read().strip().split("\n") )
    file_list.extend(open(os.path.join(data_dir, "All/test_file_list.txt")).read().strip().split("\n") )

    #Set up multiprocessing
    #multiprocessing tools
    n = len(file_list)
    h = int(n / parallelism)

    manager = mp.Manager()
    pool = mp.Pool()
    JobQueue = manager.Queue()
    jobs = []

    #multiprocessing begins
    save_file = os.path.join(save_dir, "Sentences.txt")
    json_writer = pool.apply_async(json_sentence_writer, (save_file, JobQueue))

    for i in range(0, int(1.0*n/h)):
        s = i*h; e = min((i+1) * h, n);
        job = pool.apply_async(json_sentence_extractor, (file_list[s:e], JobQueue))
        jobs.append(job)

    for job in jobs:
        job.get()

    JobQueue.put("kill")
    json_writer.get()
    pool.close()
    pool.join()

def parallel_embedding_extractor(dataset, s, e, device):
    save_file = os.path.join("../Data/Processed_Data", dataset, "pkl_files/tmp/", str(s))

    cmd = "python get_sentence_embeddings.py --dataset {} --s {} --e {} --save_file {} --{}".format(
                                                dataset, str(s), str(e), save_file, device)

    os.system(cmd)

def get_sentence_embeddings(dataset, parallelism, device):
    Sentences = open("../Data/Processed_Data/" + dataset + "/pkl_files/Sentences.txt").read().strip().split("\n")
    n = len(Sentences)
    h = 1 + int(1.0 * n/parallelism)

    manager = mp.Manager()
    pool = mp.Pool()
    JobQueue = manager.Queue()
    jobs = []

    for i in range(0, parallelism):
        s = i * h
        e = min((i+1) * h, n)
        job = pool.apply_async(parallel_embedding_extractor, (dataset, s, e, device))
        jobs.append(job)

    for job in jobs:
        job.get()

    pool.close()
    pool.join()

    save_file =


if __name__ == "__main__":
    args = argparse.ArgumentParser()
    args.add_argument("--dataset", type=str, default="nyt")
    args.add_argument("--test_split", type=float, default=0.35)
    args.add_argument("--val_split", type=float, default=0.05)
    args.add_argument("--cpu", action = "store_true")
    args.add_argument("--parallelism", type=int, default=4)
    opts = args.parse_args()

    PER_TEST_FILES = opts.test_split * 100.0
    PER_VAL_FILES = opts.val_split * 100.0

    device = 'cpu' if opts.cpu else 'gpu'

    if opts.dataset == "nyt":
        get_file_list("../Data/Datasets/nyt/pair_sent_matched/", "nyt")
        get_sentences("nyt", opts.,parallelism)
        get_sentence_embeddings("nyt", opts.parallelism , device)

